üöÄ AI Microservice Queueing SystemThis project implements a scalable, decoupled microservice architecture for handling computationally expensive, asynchronous tasks (AI summarization) without blocking the user-facing API. It demonstrates proficiency in Node.js (NestJS), Python, Redis, and Docker for containerization and orchestration.üéØ Architectural GoalsAsynchronous Processing: The API client receives an immediate response, while the time-consuming AI task runs in the background.Decoupling: Separate the responsibilities of API ingestion (NestJS) and heavy computation (Python) to maximize performance and scalability.Polyglot Communication: Enable two services written in different languages (TypeScript/Node.js and Python) to communicate reliably via a message broker.üß± Architecture DiagramThe system is composed of three core services orchestrated by Docker Compose.üõ†Ô∏è Technology StackComponentTechnologyRoleAPI Gateway (Producer)NestJS (TypeScript/Node.js)Receives client requests (REST API), validates the data, and pushes the job onto the message queue (LPUSH). It also listens for the completion status via Pub/Sub.Message BrokerRedisCentral communication hub, used both as a reliable Task Queue (job_queue List) and a non-blocking Result Channel (job_result Pub/Sub).AI Worker (Consumer)PythonPulls tasks off the queue (BRPOP), interacts with the Google Gemini API for AI summarization, and publishes the result back to Redis.OrchestrationDocker & Docker ComposeContainerizes all services and manages network communication, ensuring a portable, production-ready environment.‚öôÔ∏è Communication Flow (The Asynchronous Loop)The entire process involves two distinct, non-blocking steps mediated by Redis:Job Submission (API $\rightarrow$ Queue):Client sends POST request to NestJS /process.NestJS immediately executes LPUSH job_queue with the text payload.NestJS returns {"status": "queued"} to the client (instantaneous).Job Processing & Notification (Worker $\rightarrow$ Listener):The Python Worker executes BRPOP job_queue, pulls the task, and calls the Gemini API.Python Worker receives the summary and executes PUBLISH job_result with the final JSON response.The NestJS Listener, which is subscribed to the job_result channel, receives the packet and logs the completion.üöÄ Setup and RunPrerequisitesDocker and Docker Compose installed.A Google Gemini API Key.StepsClone the Repository (If applicable):Bashgit clone [your-repo-link]
cd redis-app
Create the .env file:In the project root directory, create a file named .env and paste your API key:# .env
GEMINI_API_KEY="YOUR_ACTUAL_GEMINI_API_KEY"
REDIS_HOST="redis" # Keep this as 'redis' for Docker network resolution
(Note: This file is excluded from version control via .gitignore for security).Build and Run the System:This command builds the Node.js and Python images, creates the network, and starts all three containers (redis-broker, nest-api, and ai-worker).Bashdocker-compose up --build
Execute the Test Request:Open a new terminal tab and run the following curl command. The terminal running docker-compose up will show the full processing loop across the services.Bashcurl -X POST http://localhost:3000/process \
   -H "Content-Type: application/json" \
   -d '{"text": "This is a long article about software engineering principles and the importance of using microservices for complex tasks."}'
üìà ScalabilityThis architecture is horizontally scalable by design:API Gateway: To handle more incoming traffic, simply scale the number of nest-producer containers. They all point to the same Redis instance.Worker Pool: To process jobs faster, simply scale the number of ai-worker containers. Since the task is popped using BRPOP, Redis guarantees that each job is picked up by only one worker, ensuring high throughput and zero duplicated effort.